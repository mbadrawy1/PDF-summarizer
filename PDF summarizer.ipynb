{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9WJn1lWn1xB",
        "outputId": "dbe8be06-5612-458f-90b6-0a2b1a65ad94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kft22VQu0tFT",
        "outputId": "c486a7f8-1fef-426b-95de-2bcc01f42fcf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.2.2)\n",
            "Building wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=ef49791059a1a237b03df359919217f38e489ae6a56d82402fb349b41b74fa15\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=c4556096ccd744a2ea0d99989196c4a14ddb26301c2242a2924f0e026983ee4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-23.12.11 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt-T1ocXlqDt",
        "outputId": "42f315ad-f43a-4176-94e7-7e32ddc3d121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities: [('Kai-wei Chang \\nUCL', 'PERSON'), ('Standford', 'PERSON'), ('CS124', 'PERSON'), ('Islam ElKabani', 'PERSON'), ('Christine Basta\\n•Email', 'PERSON'), ('3InstructorsContent', 'CARDINAL'), ('•POS Tagging\\n• Dependency', 'WORK_OF_ART'), ('• Neural Language Models', 'PRODUCT'), ('20%', 'PERCENT'), ('Assignments+ Quizzes', 'PERSON'), ('20%', 'PERCENT'), ('20%', 'PERCENT'), ('10%', 'PERCENT'), ('40%', 'PERCENT'), ('• Zero', 'PRODUCT'), ('more than one', 'CARDINAL'), ('25%', 'PERCENT'), ('PolicyWhy', 'ORG'), ('NLP', 'ORG'), ('Intelligence', 'ORG'), ('today', 'DATE'), ('Phil Blunson', 'PERSON'), ('Deep Mind', 'PERSON'), ('Oxford', 'ORG'), ('NLP', 'ORG'), ('11What', 'CARDINAL'), ('NLP', 'ORG'), ('• Identify', 'PRODUCT'), ('Beyond the Text\\n13NLP', 'WORK_OF_ART'), ('14Machine', 'CARDINAL'), ('Translation', 'ORG'), ('15Language', 'CARDINAL'), ('IBM', 'ORG'), ('Watson\\nhttps://www.youtube.com/watch?v=WFR3lOm_xhEInformation Extraction', 'ORG'), ('January 15, 2012', 'DATE'), ('Dan Jurafsky', 'PERSON'), ('Dan', 'PERSON'), ('Gates', 'ORG'), ('159', 'CARDINAL'), ('tomorrow', 'DATE'), ('10:00-11:30', 'TIME'), ('Calendar', 'PRODUCT'), ('Jan-16-2012', 'PERSON'), ('159Challenges', 'CARDINAL'), ('21•', 'CARDINAL'), ('• Word', 'PRODUCT'), ('• Syntactic', 'PRODUCT'), ('22Challenges', 'CARDINAL'), ('23Challenges', 'CARDINAL'), ('• Pronoun', 'PRODUCT'), ('• Language', 'PRODUCT'), ('25Challenges', 'CARDINAL'), ('26Challenges', 'CARDINAL'), ('27Challenges', 'CARDINAL'), ('• Different', 'PRODUCT'), ('• Different', 'ORG'), ('Neutral', 'ORG'), ('• Examples', 'PRODUCT'), ('James', 'PERSON'), ('~700K\\n', 'PERSON'), ('• Newswire', 'ORG'), ('500M+', 'CARDINAL'), ('• Wikipedia', 'FAC'), ('2.9 billion', 'CARDINAL'), ('English', 'LANGUAGE'), ('several billions', 'MONEY'), ('28Challenges', 'CARDINAL'), ('29•', 'CARDINAL'), ('30Parts', 'CARDINAL'), ('Speech', 'ORG'), ('NLP', 'ORG'), ('NLP', 'ORG'), ('• Word', 'PRODUCT'), ('• Sentence boundary detection\\n• Part-of-speech (', 'WORK_OF_ART'), ('noun', 'PERSON'), ('• Named Entity (', 'WORK_OF_ART'), ('•', 'PRODUCT'), ('• Parsing\\n•', 'PRODUCT'), ('• Semantic', 'ORG'), ('•', 'PRODUCT'), ('31NLP', 'CARDINAL'), ('NER', 'ORG'), ('NER', 'ORG'), ('U.N.', 'ORG'), ('Ekeus', 'PERSON'), ('Baghdad', 'GPE'), ('Parsing\\nSemantics\\n• Every fifteen minutes', 'WORK_OF_ART'), ('• Our', 'PRODUCT'), ('• The task', 'PRODUCT'), ('• Knowledge', 'PRODUCT'), ('• Knowledge', 'PRODUCT'), ('• A', 'PRODUCT'), ('• Luckily', 'ORG'), ('half', 'CARDINAL'), ('36What', 'CARDINAL'), ('Transformer', 'ORG'), ('OpenAI', 'GPE'), ('November 2022', 'DATE'), ('OpenAI', 'GPE'), ('GPT-3', 'PRODUCT'), ('Bard', 'PERSON'), ('Google', 'ORG'), (\"Google's Language \\nModel for Dialogue Application\", 'ORG'), ('45', 'CARDINAL')]\n",
            "Dates: ['today', 'January 15, 2012', 'tomorrow', 'November 2022']\n",
            "Summary: What is NLP? •Wiki: Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human ( natural) languages. • Bard is Google's artificial intelligence chatbot.\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "import spacy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "def extract_and_summarize(pdf_path):\n",
        "  # Extract text from PDF\n",
        "  with open(pdf_path, 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "\n",
        "    full_text = \"\"\n",
        "    for page_num in range(num_pages):\n",
        "      page = pdf_reader.pages[page_num]\n",
        "      page_text = page.extract_text()\n",
        "      full_text += page_text\n",
        "\n",
        "  # Load spaCy model for entity recognition\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(full_text)\n",
        "\n",
        "  # Extract entities\n",
        "  entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "  # Extract dates\n",
        "  dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
        "\n",
        "  # Summarize the text using Sumy LexRank\n",
        "  parser = PlaintextParser.from_string(full_text, Tokenizer(\"english\"))\n",
        "  summarizer = LexRankSummarizer()\n",
        "  summary = summarizer(parser.document, 3)  # Generate a 3-sentence summary\n",
        "  summary_text = \" \".join(str(sentence) for sentence in summary)\n",
        "\n",
        "  return entities, dates, summary_text\n",
        "\n",
        "# Example usage (assuming you uploaded the PDF to Colab)\n",
        "entities, dates, summary = extract_and_summarize(\"/content/Lec1_intro.pdf\")\n",
        "print(\"Entities:\", entities)\n",
        "print(\"Dates:\", dates)\n",
        "print(\"Summary:\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Sh-8LiMn7WE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}